{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mCzS_l4qYvk5",
        "-ZSRt3Z9ifRx",
        "GnGSd2bHofWB",
        "p5iMm4EKRkVp",
        "ky9wFFdn7r_Y",
        "pbhh3psm6XGo",
        "ZZhXJon0_woH",
        "DX6vTucBjqyh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "mCzS_l4qYvk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction of a categorical dependent variable from a number of independent variables\n",
        "# Linear_regression equation -> sigmoid function -> logistic regression function\n",
        "# Returns the probability of an outcome wrt a number of factors\n",
        "# Maximum likelihood principle -> Estimation technique used to determine the values for the parameters of a model\n",
        "# Probability density function - gaussian distribution function\n",
        "# The parameters of a model are chosen such that they maximise the likelihood in a process of a model\n",
        "# It is a generalized linear model not because of the linear classifier as a response, it is because the logit of probability estimation function response of a linear combination of the parameters\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "# find null values and fill the data value with mean values \n",
        "dataset.isnull().sum()\n",
        "# split the dataset\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "# feature scaling is required incase of logistic regression\n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "# build the model\n",
        "classifier=LogisticRegression(random_state=0) # random_state -> No need to be specified!\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predict the value\n",
        "classifier.predict(sc.transform([[19,19000]]))\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "# comparison with the actual y values\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)), axis=1)\n",
        "\n",
        "# now get the confusion matrix\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "classifier.score(x_train,y_train)\n",
        "classifier.score(x_test,y_test)\n",
        "# accuracy score --> the rate of correct predictions\n",
        "\n",
        "#r2_score(y_test,y_pred) # goodness of fit is calculated for regression models\n",
        "# Higher dimensionality is not visulaized here in 2d \n",
        "cm,accuracy_score(y_test,y_pred)\n"
      ],
      "metadata": {
        "id": "KMKXpN4UY0fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set,y_set=sc.inverse_transform(x_test),y_test\n",
        "\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.25), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000,0.25))\n",
        "\n",
        "plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape),cmap=ListedColormap(('red', 'green')),alpha=0.5)\n",
        "for i,j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i), label=j)\n",
        "plt.title('Logistic Regression (Test set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W532UXQkhMiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression on digits dataset"
      ],
      "metadata": {
        "id": "-ZSRt3Z9ifRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "import seaborn as sn \n",
        "# seaborn is used for random distributions\n",
        "# load the digits directory\n",
        "digits=load_digits()\n",
        "print(dir(digits))\n",
        "#plt.gray() # Make the picture background to gray\n",
        "#plt.matshow(digits.images[0])\n",
        "#digits.target\n",
        "x_train, x_test,y_train, y_test=train_test_split(digits.data,digits.target, test_size=0.25, random_state=0)\n",
        "\n",
        "classifier=LogisticRegression()\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "classifier.predict(digits.data[0:5])\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "\n",
        "#classifier.score(x_test,y_test)\n",
        "# visualizing confusion matrix using seaborn\n",
        "#plt.matshow(cm) # Data is not mentioned\n",
        "\n",
        "plt.figure(figsize=(20,15))\n",
        "# array is visualized/pictured like graph plots\n",
        "sn.heatmap(cm, annot=True)\n",
        "plt.xlabel(\"Predictions\")\n",
        "plt.ylabel(\"Truth\")\n",
        "plt.show()\n",
        "accuracy_score(y_test, y_pred),classifier.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "hWSOZTqhoBEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate K-fold cross validation score\n"
      ],
      "metadata": {
        "id": "jY5TvKSl0ta9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "# we get the specified number of accuracies \n",
        "accuracies=cross_val_score(classifier, x_train, y_train, cv=10) # scoring can be specified -- it means calculating the rmse or mse and other calculations\n",
        "# model object ,x data, y data and cross validate number\n",
        "accuracies.max(),np.max(accuracies),accuracies.mean()*100,accuracies.std()*100"
      ],
      "metadata": {
        "id": "GaIyIzdI0ztH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K- Nearest Neighbors"
      ],
      "metadata": {
        "id": "GnGSd2bHofWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This model has number of algorithms(brute force and efficient algo) to decide whether a new point falls within a specific category in diff number of categories \n",
        "# K nearest neighbors is not a linear classifier -- we get a curve involving the knn model \n",
        "# process\n",
        "# choose k number\n",
        "# take the k nearest neighbors using any distance method\n",
        "# count the number of neighbors falls within a category\n",
        "# Assign the new data point in the category of most number of neighbors\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sn\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "# feature scaling is required incase of KNN\n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "# now build the model\n",
        "classifier=KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "\n",
        "accuracy_score(y_test,y_pred)\n",
        "plt.figure(figsize=(10,10))\n",
        "sn.heatmap(cm, annot=True)\n",
        "plt.xlabel(\"Predictions\")\n",
        "plt.ylabel(\"Truth\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "17GCg7SqokP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization using a contour graph"
      ],
      "metadata": {
        "id": "c0vCy0_B51mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = sc.inverse_transform(x_test), y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1), np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\n",
        "\n",
        "\n",
        "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
        "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('K-NN (Test set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r0IfoJw96Aq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "p5iMm4EKRkVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The support vector machine has a set of algorithms -- Linear, polynomial, rbf, sigmoid\n",
        "# It is used to determine the hyperplane in a n dimensional space that distinctly classify the data points\n",
        "# kernel def1- makes the inseperable problem to seperable problem\n",
        "# Kernel def2- uses the linear classifier to solve non linear classification problem\n",
        "# It is used when a marginalised error is bieng considered\n",
        "# Maximum margin - The maximum marginal error of the nearest neighbors from each classification\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sn\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "# feature scaling is required incase of logistic regression \n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "# build the model\n",
        "classifier=SVC(kernel='linear', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "cm,accuracy_score(y_test,y_pred)\n"
      ],
      "metadata": {
        "id": "4F5d9BosRp0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of test set\n"
      ],
      "metadata": {
        "id": "MsgJ19012rq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The above algo is better than logistic regression \n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set,y_set=sc.inverse_transform(x_test), y_test\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))\n",
        "\n",
        "plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)\n",
        "\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)\n",
        "\n",
        "plt.xlim(x1.min(), x1.max())\n",
        "plt.ylim(x2.min(),x2.max())\n",
        "plt.xlabel(\"Age \")\n",
        "plt.ylabel(\"Estimated Salary\")\n",
        "plt.title(\"SVM (Linear function)\")\n",
        "plt.legend(title=\"categories\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d44DM8aq2vZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel SVM"
      ],
      "metadata": {
        "id": "ky9wFFdn7r_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It works on the kernel trick - which generally transforms the training data so that a non linear decision boundary is able to tranform to a linear eqn in a higher dimension of spaces\n",
        "# or map the 2d space to 3d space and classify the data points using a linear classifier (In 2d - line , 3d- hyperplane) and project back to 2d space and specify a non linear boundary \n",
        "\n",
        "# Generally kernel svm comes in to picture when a non linear seperable data set occurs\n",
        "# mapping to a higher dimension solves the problem - but it is computationally intensive\n",
        "# So kernel functions solves the problem\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sn\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "# feature scaling is required incase of logistic regression \n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "# build the model\n",
        "classifier=SVC(kernel='rbf', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm,accuracy_score(y_test,y_pred))\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set,y_set=sc.inverse_transform(x_test), y_test\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))\n",
        "\n",
        "plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)\n",
        "\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)\n",
        "\n",
        "plt.xlim(x1.min(), x1.max())\n",
        "plt.ylim(x2.min(),x2.max())\n",
        "plt.xlabel(\"Age \")\n",
        "plt.ylabel(\"Estimated Salary\")\n",
        "plt.title(\"Kernel SVM (rbf function)\")\n",
        "plt.legend(title=\"categories\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KmYXBhX4nIpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid function "
      ],
      "metadata": {
        "id": "K8Yi4t6htC5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sn\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "# feature scaling is required incase of logistic regression \n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "# build the model\n",
        "classifier=SVC(kernel='sigmoid', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm,accuracy_score(y_test,y_pred))\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set,y_set=sc.inverse_transform(x_test), y_test\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))\n",
        "\n",
        "plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)\n",
        "\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)\n",
        "\n",
        "plt.xlim(x1.min(), x1.max())\n",
        "plt.ylim(x2.min(),x2.max())\n",
        "plt.xlabel(\"Age \")\n",
        "plt.ylabel(\"Estimated Salary\")\n",
        "plt.title(\"Kernel SVM (sigmoid function)\")\n",
        "plt.legend(title=\"categories\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ERykhl29tJWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomical - kernel function"
      ],
      "metadata": {
        "id": "LlG00z-LtPz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sn\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "# feature scaling is required incase of logistic regression \n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "# build the model\n",
        "classifier=SVC(kernel='poly', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm,accuracy_score(y_test,y_pred))\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set,y_set=sc.inverse_transform(x_test), y_test\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))\n",
        "\n",
        "plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)\n",
        "\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)\n",
        "\n",
        "plt.xlim(x1.min(), x1.max())\n",
        "plt.ylim(x2.min(),x2.max())\n",
        "plt.xlabel(\"Age \")\n",
        "plt.ylabel(\"Estimated Salary\")\n",
        "plt.title(\"Kernel SVM (poly function)\")\n",
        "plt.legend(title=\"categories\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0faRnQgrtV9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classification"
      ],
      "metadata": {
        "id": "pbhh3psm6XGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sn\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "\n",
        "# build the model\n",
        "classifier=DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm,accuracy_score(y_test,y_pred))\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set,y_set=x_test, y_test\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))\n",
        "\n",
        "plt.contourf(x1,x2,classifier.predict((np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)\n",
        "\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)\n",
        "\n",
        "plt.xlim(x1.min(), x1.max())\n",
        "plt.ylim(x2.min(),x2.max())\n",
        "plt.xlabel(\"Age \")\n",
        "plt.ylabel(\"Estimated Salary\")\n",
        "plt.title(\"Decision Tree Classifier\")\n",
        "plt.legend(title=\"categories\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kCM4aTIc7IYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Regression"
      ],
      "metadata": {
        "id": "ZZhXJon0_woH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sn\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:, :-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)\n",
        "\n",
        "# build the model\n",
        "classifier=RandomForestClassifier(n_estimators=10,criterion='entropy', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "# predictions\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm,accuracy_score(y_test,y_pred))\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set,y_set=x_test, y_test\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))\n",
        "\n",
        "plt.contourf(x1,x2,classifier.predict((np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)\n",
        "\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)\n",
        "\n",
        "plt.xlim(x1.min(), x1.max())\n",
        "plt.ylim(x2.min(),x2.max())\n",
        "plt.xlabel(\"Age \")\n",
        "plt.ylabel(\"Estimated Salary\")\n",
        "plt.title(\"Random Forest Classification\")\n",
        "plt.legend(title=\"categories\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E1YKIIj8_z4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classification"
      ],
      "metadata": {
        "id": "DX6vTucBjqyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It is often called as naive classification on the reason of independence assumption which means the features that value to the outcome must be independent \n",
        "# This method works on the principle of Bayes theorem which is a mathematical formula for conditional probability\n",
        "# Bayes theorem p(A|B)=(p(B|A)*p(A))/p(B)\n",
        "# This method can be used in case of correlated features \n",
        "# The Highest probability of outcome of a category will serve the answer for this classification model\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, r2_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sn\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "dataset=pd.read_csv(\"Social_Network_Ads.csv\")\n",
        "x=dataset.iloc[:,:-1].values\n",
        "y=dataset.iloc[:,-1].values\n",
        "\n",
        "imp=SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "x=imp.fit_transform(x)\n",
        "\n",
        "x_train, x_test,y_train,y_test=train_test_split(x,y,test_size=0.25, random_state=0)\n",
        "\n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "classifier=GaussianNB()\n",
        "classifier.fit(x_train,y_train)\n",
        "\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)), axis=1)\n",
        "\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "accuracy_score(y_test,y_pred)\n",
        "#plt.subplot(1,2,1)\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "'''\n",
        "sn.heatmap(cm, annot=True)\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Truth\")\n",
        "plt.legend([\"Predictions\", 'truth'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "'''\n",
        "x_set,y_set=sc.inverse_transform(x_test),y_test\n",
        "x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10, 0.25), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.25))\n",
        "plt.contourf(x1,x2, classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')),alpha=0.75)\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "  plt.scatter(x_set[y_set==j,0], x_set[y_set==j, 1], c=ListedColormap(('red','green'))(i), label=j)\n",
        "plt.xlim(x1.min(), x1.max())\n",
        "plt.ylim(x2.min(),x2.max())\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Salary\")\n",
        "plt.title(\"Naive Bayes\")\n",
        "plt.legend(\"Categories\")\n",
        "plt.suptitle(\"Confusion matrix and Contour plot for classification\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DnS_HTrNk0DI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}