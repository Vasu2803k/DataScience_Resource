# -*- coding: utf-8 -*-
"""Deep_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1meFNB4Y0Ee7FzfSHq28nmSCrlExlnZQp

# Deep Learning

Deep learning is a specialised field of ML that relies on learning with neural networks

# Artificial Neural Network
"""

# Artificial neural network is based on biological neurons
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score

dataset=pd.read_csv("Churn_Modelling.csv")
x=dataset.iloc[:,3:-1]
y=dataset.iloc[:,-1]

# Encoding the categorical data
# Geography
# One hot encoding
ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])], remainder='passthrough')
x=ct.fit_transform(x)

# Gender
# Label Encoding
le=LabelEncoder()
x[:,4]=le.fit_transform(x[:,4])

# Splitting
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state=0)

# Feature scaling
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

# ann initialization
# ann object -> using tensorflow - keras - models.Sequential
ann=tf.keras.models.Sequential()

# Adding the input and first hidden layer
# 6 number of neurons
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
# Adding the 2nd hidden layer
# 6 number of neurons
ann.add(tf.keras.layers.Dense(units=6, activation='relu'))
# Adding the output layer
# 1 neuron - depends on number of category of values
ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# layers are added 
# compiling the cnn - specify the cnn optimizser and loss value so that we obtain the required dependent variable
# adam optimizer is a replacement for stochastic gradient descent
ann.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])
# loss = if binary value --> binary_crossentropy
# loss = if more than 2 --> categorical_crossentropy

# Training
ann.fit(x_train,y_train,batch_size=32, epochs=100)
# Thumb rule batch size = 32
# epochs= No of iterations of all the training data in one cycle for training the model

y_pred=ann.predict(x_test)>0.5
cm=confusion_matrix(y_test, y_pred)
cm
accuracy_score(y_test,y_pred)

"""# Convolutional Neural Network"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator

# Data Agumentation -- Increasing the amount of data by adding slightly modified copies of already existing data
# CNN - Convolution(+ adding Relu) -> pooling -> flattening -> ANN (full connection)
# Input image [convolution process with ] Feature detector -> feature map
# input image-feature detector --> Convolution layer (feature maps) --> Pooling layer


# create the training and test set of images
train_data_gen=ImageDataGenerator(rescale=1.0/255, zoom_range=0.2, shear_range=0.2, horizontal_flip=True ) # Agumentation is done to make the cnn learn well
training_set=train_data_gen.flow_from_directory('file_path', target_size=[64,64], batch_size=32, class_mode='binary')

test_data_gen=ImageDataGenerator(rescale=1.0/255)
testing_set=test_data_gen.flow_from_directory('file_path', target_size=[64,64],batch_size=32, class_mode='bianry')

# Building the cnn
cnn=tf.keras.models.Sequential()

# 1st convolutional layer
cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,activation='relu', input_shape=[64,64,3]))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# 2nd convolutional layer
cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3,activation='relu', input_shape=[64,64,3]))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# flattening
cnn.add(tf.keras.layers.Flatten())

# adding the hidden layer - fully connected layer (full connection)
cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))

# output layer
cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))


# compiling the cnn
cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

cnn.fit(x=training_set, validation_data=testing_set, epochs=25)

"""Predictions"""

from keras.preprocessing import image
test_image=image.load_img('file_path',target_size=[64,64])
test_image=image.img_to_array(test_image)
test_image=np.expand_dims(test_image, axis=0)
result=cnn.predict(test_image)
training_set.indices_