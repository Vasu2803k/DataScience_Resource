# -*- coding: utf-8 -*-
"""Classification_Models_DataScience.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s96o_MlZQ9NH5L5yAVcKW7FjNcBTMlMW

# Logistic Regression
"""

# Prediction of a categorical dependent variable from a number of independent variables
# Linear_regression equation -> sigmoid function -> logistic regression function
# Returns the probability of an outcome wrt a number of factors
# Maximum likelihood principle -> Estimation technique used to determine the values for the parameters of a model
# Probability density function - gaussian distribution function
# The parameters of a model are chosen such that they maximise the likelihood in a process of a model
# It is a generalized linear model not because of the linear classifier as a response, it is because the logit of probability estimation function response of a linear combination of the parameters

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
from sklearn.model_selection import train_test_split
dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

# find null values and fill the data value with mean values 
dataset.isnull().sum()
# split the dataset
x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)
# feature scaling is required incase of logistic regression
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

# build the model
classifier=LogisticRegression(random_state=0) # random_state -> No need to be specified!
classifier.fit(x_train, y_train)

# predict the value
classifier.predict(sc.transform([[19,19000]]))
y_pred=classifier.predict(x_test)

# comparison with the actual y values
np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)), axis=1)

# now get the confusion matrix
cm=confusion_matrix(y_test,y_pred)
classifier.score(x_train,y_train)
classifier.score(x_test,y_test)
# accuracy score --> the rate of correct predictions

#r2_score(y_test,y_pred) # goodness of fit is calculated for regression models
# Higher dimensionality is not visulaized here in 2d 
cm,accuracy_score(y_test,y_pred)

from matplotlib.colors import ListedColormap
x_set,y_set=sc.inverse_transform(x_test),y_test

x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.25), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000,0.25))

plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape),cmap=ListedColormap(('red', 'green')),alpha=0.5)
for i,j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i), label=j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# Logistic regression on digits dataset"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, r2_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
import seaborn as sn 
# seaborn is used for random distributions
# load the digits directory
digits=load_digits()
print(dir(digits))
#plt.gray() # Make the picture background to gray
#plt.matshow(digits.images[0])
#digits.target
x_train, x_test,y_train, y_test=train_test_split(digits.data,digits.target, test_size=0.25, random_state=0)

classifier=LogisticRegression()
classifier.fit(x_train, y_train)

classifier.predict(digits.data[0:5])
y_pred=classifier.predict(x_test)

cm=confusion_matrix(y_test,y_pred)

#classifier.score(x_test,y_test)
# visualizing confusion matrix using seaborn
#plt.matshow(cm) # Data is not mentioned

plt.figure(figsize=(20,15))
# array is visualized/pictured like graph plots
sn.heatmap(cm, annot=True)
plt.xlabel("Predictions")
plt.ylabel("Truth")
plt.show()
accuracy_score(y_test, y_pred),classifier.score(x_test, y_test)

"""Calculate K-fold cross validation score

"""

from sklearn.model_selection import cross_val_score
# we get the specified number of accuracies 
accuracies=cross_val_score(classifier, x_train, y_train, cv=10) # scoring can be specified -- it means calculating the rmse or mse and other calculations
# model object ,x data, y data and cross validate number
accuracies.max(),np.max(accuracies),accuracies.mean()*100,accuracies.std()*100

"""# K- Nearest Neighbors"""

# This model has number of algorithms(brute force and efficient algo) to decide whether a new point falls within a specific category in diff number of categories 
# K nearest neighbors is not a linear classifier -- we get a curve involving the knn model 
# process
# choose k number
# take the k nearest neighbors using any distance method
# count the number of neighbors falls within a category
# Assign the new data point in the category of most number of neighbors

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import seaborn as sn

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)
# feature scaling is required incase of KNN
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

# now build the model
classifier=KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
classifier.fit(x_train, y_train)

y_pred=classifier.predict(x_test)

cm=confusion_matrix(y_test,y_pred)

accuracy_score(y_test,y_pred)
plt.figure(figsize=(10,10))
sn.heatmap(cm, annot=True)
plt.xlabel("Predictions")
plt.ylabel("Truth")
plt.show()

"""Visualization using a contour graph"""

from matplotlib.colors import ListedColormap
X_set, y_set = sc.inverse_transform(x_test), y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1), np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))


plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('K-NN (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

"""# SVM"""

# The support vector machine has a set of algorithms -- Linear, polynomial, rbf, sigmoid
# It is used to determine the hyperplane in a n dimensional space that distinctly classify the data points
# kernel def1- makes the inseperable problem to seperable problem
# Kernel def2- uses the linear classifier to solve non linear classification problem
# It is used when a marginalised error is bieng considered
# Maximum margin - The maximum marginal error of the nearest neighbors from each classification


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import seaborn as sn

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)
# feature scaling is required incase of logistic regression 
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

# build the model
classifier=SVC(kernel='linear', random_state=0)
classifier.fit(x_train, y_train)

# predictions
y_pred=classifier.predict(x_test)

np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)

cm=confusion_matrix(y_test,y_pred)
cm,accuracy_score(y_test,y_pred)

"""Visualization of test set

"""

# The above algo is better than logistic regression 
from matplotlib.colors import ListedColormap
x_set,y_set=sc.inverse_transform(x_test), y_test
x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))

plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)

plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(),x2.max())
plt.xlabel("Age ")
plt.ylabel("Estimated Salary")
plt.title("SVM (Linear function)")
plt.legend(title="categories")
plt.show()

"""# Kernel SVM"""

# It works on the kernel trick - which generally transforms the training data so that a non linear decision boundary is able to tranform to a linear eqn in a higher dimension of spaces
# or map the 2d space to 3d space and classify the data points using a linear classifier (In 2d - line , 3d- hyperplane) and project back to 2d space and specify a non linear boundary 

# Generally kernel svm comes in to picture when a non linear seperable data set occurs
# mapping to a higher dimension solves the problem - but it is computationally intensive
# So kernel functions solves the problem

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import seaborn as sn

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)
# feature scaling is required incase of logistic regression 
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

# build the model
classifier=SVC(kernel='rbf', random_state=0)
classifier.fit(x_train, y_train)

# predictions
y_pred=classifier.predict(x_test)

np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)

cm=confusion_matrix(y_test,y_pred)
print(cm,accuracy_score(y_test,y_pred))

from matplotlib.colors import ListedColormap
x_set,y_set=sc.inverse_transform(x_test), y_test
x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))

plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)

plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(),x2.max())
plt.xlabel("Age ")
plt.ylabel("Estimated Salary")
plt.title("Kernel SVM (rbf function)")
plt.legend(title="categories")
plt.show()

"""Sigmoid function """

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import seaborn as sn

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)
# feature scaling is required incase of logistic regression 
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

# build the model
classifier=SVC(kernel='sigmoid', random_state=0)
classifier.fit(x_train, y_train)

# predictions
y_pred=classifier.predict(x_test)

np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)

cm=confusion_matrix(y_test,y_pred)
print(cm,accuracy_score(y_test,y_pred))

from matplotlib.colors import ListedColormap
x_set,y_set=sc.inverse_transform(x_test), y_test
x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))

plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)

plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(),x2.max())
plt.xlabel("Age ")
plt.ylabel("Estimated Salary")
plt.title("Kernel SVM (sigmoid function)")
plt.legend(title="categories")
plt.show()

"""Polynomical - kernel function"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import seaborn as sn

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)
# feature scaling is required incase of logistic regression 
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

# build the model
classifier=SVC(kernel='poly', random_state=0)
classifier.fit(x_train, y_train)

# predictions
y_pred=classifier.predict(x_test)

np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)

cm=confusion_matrix(y_test,y_pred)
print(cm,accuracy_score(y_test,y_pred))

from matplotlib.colors import ListedColormap
x_set,y_set=sc.inverse_transform(x_test), y_test
x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))

plt.contourf(x1,x2,classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)

plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(),x2.max())
plt.xlabel("Age ")
plt.ylabel("Estimated Salary")
plt.title("Kernel SVM (poly function)")
plt.legend(title="categories")
plt.show()

"""# Decision Tree Classification"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import seaborn as sn

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)

# build the model
classifier=DecisionTreeClassifier(criterion='entropy', random_state=0)
classifier.fit(x_train, y_train)

# predictions
y_pred=classifier.predict(x_test)

np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)

cm=confusion_matrix(y_test,y_pred)
print(cm,accuracy_score(y_test,y_pred))

from matplotlib.colors import ListedColormap
x_set,y_set=x_test, y_test
x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))

plt.contourf(x1,x2,classifier.predict((np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)

plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(),x2.max())
plt.xlabel("Age ")
plt.ylabel("Estimated Salary")
plt.title("Decision Tree Classifier")
plt.legend(title="categories")
plt.show()

"""# Random Forest Regression"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import seaborn as sn

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:, :-1].values
y=dataset.iloc[:,-1].values

x_train,x_test,y_train, y_test=train_test_split(x,y,test_size=0.25,random_state=0)

# build the model
classifier=RandomForestClassifier(n_estimators=10,criterion='entropy', random_state=0)
classifier.fit(x_train, y_train)

# predictions
y_pred=classifier.predict(x_test)

np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)),axis=1)

cm=confusion_matrix(y_test,y_pred)
print(cm,accuracy_score(y_test,y_pred))

from matplotlib.colors import ListedColormap
x_set,y_set=x_test, y_test
x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10,0.5), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.5))

plt.contourf(x1,x2,classifier.predict((np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')), alpha=0.8)

for i, j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j, 0], x_set[y_set==j,1], c=ListedColormap(('red','green'))(i),label=j)

plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(),x2.max())
plt.xlabel("Age ")
plt.ylabel("Estimated Salary")
plt.title("Random Forest Classification")
plt.legend(title="categories")
plt.show()

"""# Naive Bayes Classification"""

# It is often called as naive classification on the reason of independence assumption which means the features that value to the outcome must be independent 
# This method works on the principle of Bayes theorem which is a mathematical formula for conditional probability
# Bayes theorem p(A|B)=(p(B|A)*p(A))/p(B)
# This method can be used in case of correlated features 
# The Highest probability of outcome of a category will serve the answer for this classification model

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, r2_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import seaborn as sn
from matplotlib.colors import ListedColormap
from sklearn.naive_bayes import GaussianNB

dataset=pd.read_csv("Social_Network_Ads.csv")
x=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

imp=SimpleImputer(missing_values=np.nan, strategy='mean')
x=imp.fit_transform(x)

x_train, x_test,y_train,y_test=train_test_split(x,y,test_size=0.25, random_state=0)

sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

classifier=GaussianNB()
classifier.fit(x_train,y_train)

y_pred=classifier.predict(x_test)

np.concatenate((y_test.reshape(len(y_test),1), y_pred.reshape(len(y_pred),1)), axis=1)

cm=confusion_matrix(y_test,y_pred)
accuracy_score(y_test,y_pred)
#plt.subplot(1,2,1)
plt.figure(figsize=(10,7))

'''
sn.heatmap(cm, annot=True)
plt.xlabel("Prediction")
plt.ylabel("Truth")
plt.legend(["Predictions", 'truth'])
plt.title("Confusion Matrix")


plt.subplot(1,2,2)
'''
x_set,y_set=sc.inverse_transform(x_test),y_test
x1,x2=np.meshgrid(np.arange(x_set[:,0].min()-10, x_set[:,0].max()+10, 0.25), np.arange(x_set[:,1].min()-1000, x_set[:,1].max()+1000, 0.25))
plt.contourf(x1,x2, classifier.predict(sc.transform(np.array([x1.ravel(), x2.ravel()]).T)).reshape(x1.shape), cmap=ListedColormap(('red','green')),alpha=0.75)
for i, j in enumerate(np.unique(y_set)):
  plt.scatter(x_set[y_set==j,0], x_set[y_set==j, 1], c=ListedColormap(('red','green'))(i), label=j)
plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(),x2.max())
plt.xlabel("Age")
plt.ylabel("Salary")
plt.title("Naive Bayes")
plt.legend("Categories")
plt.suptitle("Confusion matrix and Contour plot for classification")
plt.show()